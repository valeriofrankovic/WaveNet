{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametri mreže"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperparametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treba nadodati iznose hiperparametara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "dilation_layers = 40\n",
    "dilations = [(2 ** (a % 10)) for a in range(dilation_layers)]\n",
    "filter_width = 2\n",
    "sequence_length = 8000\n",
    "residual_channels = 32\n",
    "dilation_channels = 32\n",
    "quantization_channels = 256\n",
    "receptive_field = sum(dilations) + 1\n",
    "#use_biases = True\n",
    "#skip_channels = None\n",
    "#scalar_input = None\n",
    "#initial_filter_width = None\n",
    "#histogarms = None\n",
    "#global_condition_channels = None\n",
    "#global_condition_cardinality = None\n",
    "#receptive_field = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4092\n"
     ]
    }
   ],
   "source": [
    "print(sum(dilations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholderi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treba nadodati dimenziju plejsholdera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, sequence_length, quantization_channels])\n",
    "y = tf.placeholder(tf.float32, [None, (sequence_length - receptive_field), quantization_channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varijable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# podfunkcija za izgradnju varijable\\ndef create_variable(name, shape):\\n    initializer = tf.contrib.layers.xavier_initializer_conv2d()\\n    variable = tf.Variable(initializer(shape=shape), name=name)\\n    return variable\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# podfunkcija za izgradnju varijable\n",
    "def create_variable(name, shape):\n",
    "    initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    variable = tf.Variable(initializer(shape=shape), name=name)\n",
    "    return variable\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef build_vars(dilations):\\n    var = dict()\\n    \\n    # causal layer\\n    #TO DO\\n    layer = dict()\\n    layer['filter'] = create_variable('filter', [initial_filter_width, initial_channels, self.residual_channels])\\n    var['causal_layer'] = layer\\n    \\n    # dilated stack\\n    #TO DO\\n    for i, dilation in enumerate(self.dilations):\\n        current = dict()\\n        current['filter'] = create_variable('filter', [self.filter_width, self.residual_channels, self.dilation_channels])\\n        current['gate'] = create_variable('gate', [self.filter_width, self.residual_channels, self.dilation_channels])\\n        current['dense'] = create_variable('dense', [1, self.dilation_channels, self.residual_channels])\\n        current['skip'] = create_variable('skip', [1, self.dilation_channels, self.skip_channels])\\n    \\n        if self.use_biases:\\n            current['filter_bias'] = create_bias_variable('filter_bias', [self.dilation_channels])\\n            current['gate_bias'] = create_bias_variable('gate_bias', [self.dilation_channels])\\n            current['dense_bias'] = create_bias_variable('dense_bias', [self.residual_channels])\\n            current['skip_bias'] = create_bias_variable('slip_bias', [self.skip_channels])\\n\\n        var['dilated_stack'].append(current)\\n        \\n    # postprocessing\\n    #TO DO\\n    current = dict()\\n    current['postprocess1'] = create_variable('postprocess1', [1, self.skip_channels, self.skip_channels])\\n    current['postprocess2'] = create_variable('postprocess2', [1, self.skip_channels, self.quantization_channels])\\n    if self.use_biases:\\n        current['postprocess1_bias'] = create_bias_variable('postprocess1_bias', [self.skip_channels])\\n        current['postprocess2_bias'] = create_bias_variable('postprocess2_bias', [self.quantization_channels])\\n    var['postprocessing'] = current\\n\\n    return var\\n\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def build_vars(dilations):\n",
    "    var = dict()\n",
    "    \n",
    "    # causal layer\n",
    "    #TO DO\n",
    "    layer = dict()\n",
    "    layer['filter'] = create_variable('filter', [initial_filter_width, initial_channels, self.residual_channels])\n",
    "    var['causal_layer'] = layer\n",
    "    \n",
    "    # dilated stack\n",
    "    #TO DO\n",
    "    for i, dilation in enumerate(self.dilations):\n",
    "        current = dict()\n",
    "        current['filter'] = create_variable('filter', [self.filter_width, self.residual_channels, self.dilation_channels])\n",
    "        current['gate'] = create_variable('gate', [self.filter_width, self.residual_channels, self.dilation_channels])\n",
    "        current['dense'] = create_variable('dense', [1, self.dilation_channels, self.residual_channels])\n",
    "        current['skip'] = create_variable('skip', [1, self.dilation_channels, self.skip_channels])\n",
    "    \n",
    "        if self.use_biases:\n",
    "            current['filter_bias'] = create_bias_variable('filter_bias', [self.dilation_channels])\n",
    "            current['gate_bias'] = create_bias_variable('gate_bias', [self.dilation_channels])\n",
    "            current['dense_bias'] = create_bias_variable('dense_bias', [self.residual_channels])\n",
    "            current['skip_bias'] = create_bias_variable('slip_bias', [self.skip_channels])\n",
    "\n",
    "        var['dilated_stack'].append(current)\n",
    "        \n",
    "    # postprocessing\n",
    "    #TO DO\n",
    "    current = dict()\n",
    "    current['postprocess1'] = create_variable('postprocess1', [1, self.skip_channels, self.skip_channels])\n",
    "    current['postprocess2'] = create_variable('postprocess2', [1, self.skip_channels, self.quantization_channels])\n",
    "    if self.use_biases:\n",
    "        current['postprocess1_bias'] = create_bias_variable('postprocess1_bias', [self.skip_channels])\n",
    "        current['postprocess2_bias'] = create_bias_variable('postprocess2_bias', [self.quantization_channels])\n",
    "    var['postprocessing'] = current\n",
    "\n",
    "    return var\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mreža"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef causal_conv(x, filter_, dilation):\\n    pass\\n\\ndef create_causal(x, variables):\\n    weights_filter = variables[\\'causal_layer\\'][\\'filter\\']\\n    return causal_conv(x, weights_filter, 1)\\n    \\ndef create_dilation(x, variables, layer_index, dilation, output_width, use_biases):\\n    variables = self.variables[\\'dilated_stack\\'][layer_index]\\n\\n    weights_filter = variables[\\'filter\\']\\n    weights_gate = variables[\\'gate\\']\\n\\n    conv_filter = causal_conv(input_batch, weights_filter, dilation)\\n    conv_gate = causal_conv(input_batch, weights_gate, dilation)\\n    \\n    if use_biases:\\n            filter_bias = variables[\\'filter_bias\\']\\n            gate_bias = variables[\\'gate_bias\\']\\n            conv_filter = tf.add(conv_filter, filter_bias)\\n            conv_gate = tf.add(conv_gate, gate_bias)\\n\\n    out = tf.tanh(conv_filter) * tf.sigmoid(conv_gate)\\n    \\n    ######\\n    #SHVATIT OD TUDA NA DOLJE\\n    #####################################\\n    \\n    # The 1x1 conv to produce the residual output\\n    weights_dense = variables[\\'dense\\']\\n    transformed = tf.nn.conv1d(out, weights_dense, stride=1, padding=\"SAME\", name=\"dense\")\\n\\n    # The 1x1 conv to produce the skip output\\n    skip_cut = tf.shape(out)[1] - output_width\\n    out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, -1])\\n    weights_skip = variables[\\'skip\\']\\n    skip_contribution = tf.nn.conv1d(out_skip, weights_skip, stride=1, padding=\"SAME\", name=\"skip\")\\n\\n    if self.use_biases:\\n        dense_bias = variables[\\'dense_bias\\']\\n        skip_bias = variables[\\'skip_bias\\']\\n        transformed = transformed + dense_bias\\n        skip_contribution = skip_contribution + skip_bias\\n        \\n    input_cut = tf.shape(input_batch)[1] - tf.shape(transformed)[1]\\n    input_batch = tf.slice(input_batch, [0, input_cut, 0], [-1, -1, -1])\\n\\n    return skip_contribution, input_batch + transformed\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def causal_conv(x, filter_, dilation):\n",
    "    pass\n",
    "\n",
    "def create_causal(x, variables):\n",
    "    weights_filter = variables['causal_layer']['filter']\n",
    "    return causal_conv(x, weights_filter, 1)\n",
    "    \n",
    "def create_dilation(x, variables, layer_index, dilation, output_width, use_biases):\n",
    "    variables = self.variables['dilated_stack'][layer_index]\n",
    "\n",
    "    weights_filter = variables['filter']\n",
    "    weights_gate = variables['gate']\n",
    "\n",
    "    conv_filter = causal_conv(input_batch, weights_filter, dilation)\n",
    "    conv_gate = causal_conv(input_batch, weights_gate, dilation)\n",
    "    \n",
    "    if use_biases:\n",
    "            filter_bias = variables['filter_bias']\n",
    "            gate_bias = variables['gate_bias']\n",
    "            conv_filter = tf.add(conv_filter, filter_bias)\n",
    "            conv_gate = tf.add(conv_gate, gate_bias)\n",
    "\n",
    "    out = tf.tanh(conv_filter) * tf.sigmoid(conv_gate)\n",
    "    \n",
    "    ######\n",
    "    #SHVATIT OD TUDA NA DOLJE\n",
    "    #####################################\n",
    "    \n",
    "    # The 1x1 conv to produce the residual output\n",
    "    weights_dense = variables['dense']\n",
    "    transformed = tf.nn.conv1d(out, weights_dense, stride=1, padding=\"SAME\", name=\"dense\")\n",
    "\n",
    "    # The 1x1 conv to produce the skip output\n",
    "    skip_cut = tf.shape(out)[1] - output_width\n",
    "    out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, -1])\n",
    "    weights_skip = variables['skip']\n",
    "    skip_contribution = tf.nn.conv1d(out_skip, weights_skip, stride=1, padding=\"SAME\", name=\"skip\")\n",
    "\n",
    "    if self.use_biases:\n",
    "        dense_bias = variables['dense_bias']\n",
    "        skip_bias = variables['skip_bias']\n",
    "        transformed = transformed + dense_bias\n",
    "        skip_contribution = skip_contribution + skip_bias\n",
    "        \n",
    "    input_cut = tf.shape(input_batch)[1] - tf.shape(transformed)[1]\n",
    "    input_batch = tf.slice(input_batch, [0, input_cut, 0], [-1, -1, -1])\n",
    "\n",
    "    return skip_contribution, input_batch + transformed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_net(inputs, filter_width, quantization_channels, dilation_channels, dilations):\n",
    "    \n",
    "    #print(\"udjen\")\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = tf.reshape(inputs, [-1, sequence_length, quantization_channels])\n",
    "    \n",
    "    # Izračun \n",
    "    output_width = tf.shape(inputs)[1] - receptive_field\n",
    "    \n",
    "    # Causal layer\n",
    "    input_layer = tf.layers.conv1d(inputs=input_layer, filters=dilation_channels, kernel_size=filter_width)\n",
    "    \n",
    "    #print(\"pasan\")\n",
    "    \n",
    "    hidden_layers = dict()\n",
    "    # Hidden layers\n",
    "    for i in range(dilation_layers):\n",
    "        #print(\"brakepoint 1\")\n",
    "        hidden_layers[i] = dict()\n",
    "        hidden_layers[i]['filter'] = tf.tanh(tf.layers.conv1d(inputs=input_layer, filters=dilation_channels, kernel_size=filter_width, dilation_rate=dilations[i]))\n",
    "        #tf.Print(\"brakepoint 3\")\n",
    "        hidden_layers[i]['gate'] = tf.sigmoid(tf.layers.conv1d(inputs=input_layer, filters=dilation_channels, kernel_size=filter_width, dilation_rate=dilations[i]))\n",
    "        #tf.Print(\"brakepoint 4\")\n",
    "        temp_multiplication = tf.multiply(hidden_layers[i]['filter'], hidden_layers[i]['gate'])\n",
    "        #tf.Print(\"brakepoint 5\")\n",
    "        out = tf.layers.conv1d(inputs=temp_multiplication, filters=dilation_channels, kernel_size=1, padding='same')\n",
    "        #print(\"brakepoint 6\")\n",
    "        input_cut = tf.shape(input_layer)[1] - tf.shape(out)[1]\n",
    "        #print(\"brakepoint 7\")\n",
    "        input_layer = tf.slice(input_layer, [0, input_cut, 0], [-1, -1, -1])\n",
    "        #tf.Print(\"brakepoint 8\")\n",
    "        input_layer = input_layer + out\n",
    "        #print(\"brakepoint 9\")\n",
    "        skip_cut = tf.shape(out)[1] - output_width\n",
    "        #print(\"brakepoint 10\")\n",
    "        #tf.Print(tf.shape(out)[1], [tf.shape(out)[1]])\n",
    "        #tf.Print(output_width, [output_width])\n",
    "        hidden_layers[i]['skip'] = tf.slice(out, [0, skip_cut, 0], [-1, -1, -1])\n",
    "        #print(\"brakepoint 11\")\n",
    "    \n",
    "    accumulation = hidden_layers[0]['skip']\n",
    "    for i in range(1, dilation_layers):\n",
    "        accumulation = tf.add(accumulation, hidden_layers[i]['skip'])\n",
    "    \n",
    "    relu1 = tf.nn.relu(accumulation)\n",
    "    conv1 = tf.layers.conv1d(inputs=relu1, filters=512, kernel_size=1)\n",
    "    relu2 = tf.nn.relu(conv1)\n",
    "    conv2 = tf.layers.conv1d(inputs=relu2, filters=256, kernel_size=1)\n",
    "    \n",
    "    \n",
    "    return conv2\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    outputs = []\n",
    "    current_layer = inputs\n",
    "    \n",
    "    initial_channels = 1    # treba videt parametre\n",
    "    \n",
    "    \n",
    "    \n",
    "    # causal layer\n",
    "    current_layer = create_causal(current_layer)\n",
    "    output_width = tf.shape(inputs)[1] - receptive_field + 1 # SHVATIT OVU LINIJU KODA\n",
    "    \n",
    "    # dilation layer\n",
    "    for layer_index, dilation in enumerate(dilations):\n",
    "        output, current_layer = create_dilation(current_layer, variables, layer_index, dilation, output_width, use_biases)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # postprocessing\n",
    "    w1 = self.variables['postprocessing']['postprocess1']\n",
    "    w2 = self.variables['postprocessing']['postprocess2']\n",
    "    if use_biases:\n",
    "        b1 = self.variables['postprocessing']['postprocess1_bias']\n",
    "        b2 = self.variables['postprocessing']['postprocess2_bias']\n",
    "    \n",
    "    total = sum(outputs)\n",
    "    transformed1 = tf.nn.relu(total)\n",
    "    conv1 = tf.nn.conv1d(transformed1, w1, stride=1, padding=\"SAME\")\n",
    "    if self.use_biases:\n",
    "        conv1 = tf.add(conv1, b1)\n",
    "    transformed2 = tf.nn.relu(conv1)\n",
    "    conv2 = tf.nn.conv1d(transformed2, w2, stride=1, padding=\"SAME\")\n",
    "    if self.use_biases:\n",
    "        conv2 = tf.add(conv2, b2)\n",
    "\n",
    "    return conv2\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef loss(input_batch, l2_regularization_strength=None):\\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=target_output)\\n    reduced_loss = tf.reduce_mean(loss)\\n    return reduced_loss\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def loss(input_batch, l2_regularization_strength=None):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=target_output)\n",
    "    reduced_loss = tf.reduce_mean(loss)\n",
    "    return reduced_loss\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tu treba dodat optimizer\n",
    "logits = build_net(x, filter_width, quantization_channels, dilation_channels, dilations)\n",
    "prediction = tf.nn.softmax(logits=logits)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treniranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0,   0,   0, ...,   0,   0,   0],\n",
       "         [  3,   0, 254, ..., 255,   3,   0],\n",
       "         [  4,   0, 252, ..., 255,   1,   0],\n",
       "         [  2,   0, 255, ..., 255,   4,   0],\n",
       "         [  2,   0, 252, ..., 255,   3,   0]],\n",
       "\n",
       "        [[  0,   0,   0, ...,   0,   0, 255],\n",
       "         [  0, 254, 255, ...,   3,   0,   0],\n",
       "         [  0, 252, 255, ...,   1,   0, 255],\n",
       "         [  0, 255, 255, ...,   4,   0, 253],\n",
       "         [  0, 252, 255, ...,   3,   0, 254]]],\n",
       "\n",
       "\n",
       "       [[[255, 255,   2, ...,   0, 255, 255],\n",
       "         [  0,   0,   3, ...,   0, 254, 255],\n",
       "         [255, 255,   0, ...,   0, 253, 255],\n",
       "         [253, 255,   6, ...,   0, 255, 255],\n",
       "         [254, 255, 255, ...,   0,   0,   0]],\n",
       "\n",
       "        [[255,   2,   0, ..., 255, 255,   3],\n",
       "         [  0,   3,   0, ..., 254, 255,   4],\n",
       "         [255,   0,   0, ..., 253, 255,   2],\n",
       "         [255,   6,   0, ..., 255, 255,   2],\n",
       "         [255, 255, 255, ...,   0,   0,   4]]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Preproc('/Users/valerio/Music/nazoF5QROxPS.128.raw', batch_size, sequence_length)\n",
    "ds.preprocess()\n",
    "ds.create_minibatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0, 1, 2, 3]])\n",
    "print(np.argmax(a))\n",
    "e, x_lsls, y_lsls = ds.next_minibatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 8000)\n"
     ]
    }
   ],
   "source": [
    "print(x_lsls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(inputs, quantization_channels):\n",
    "    res = np.zeros((inputs.shape[0], inputs.shape[1], quantization_channels))\n",
    "    for i in range(inputs.shape[0]):\n",
    "        for j in range(inputs.shape[1]):\n",
    "            res[i, j, inputs[i, j]] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 loss: 5.48986 duration: 6.868599891662598\n",
      "Epoch: 0 finished! Average loss is 5.48986053467 Average time is 6.868599891662598\n",
      "Step: 0 loss: 4.99703 duration: 5.303632020950317\n",
      "Step: 1 loss: 4.21001 duration: 4.8583738803863525\n",
      "Epoch: 1 finished! Average loss is 4.60352063179 Average time is 5.081002950668335\n",
      "Step: 0 loss: 3.13336 duration: 5.565047025680542\n",
      "Step: 1 loss: 2.50405 duration: 5.001895189285278\n",
      "Epoch: 2 finished! Average loss is 2.81870746613 Average time is 5.28347110748291\n",
      "Step: 0 loss: 2.37078 duration: 4.751143932342529\n",
      "Step: 1 loss: 2.27066 duration: 5.6316819190979\n",
      "Epoch: 3 finished! Average loss is 2.32072126865 Average time is 5.191412925720215\n",
      "Step: 0 loss: 2.12469 duration: 5.721787929534912\n",
      "Step: 1 loss: 2.14274 duration: 5.079869270324707\n",
      "Epoch: 4 finished! Average loss is 2.13371419907 Average time is 5.40082859992981\n",
      "Step: 0 loss: 2.04262 duration: 6.463510036468506\n",
      "Step: 1 loss: 1.96071 duration: 5.165129899978638\n",
      "Epoch: 5 finished! Average loss is 2.00166839361 Average time is 5.814319968223572\n",
      "Step: 0 loss: 1.85871 duration: 5.456101894378662\n",
      "Step: 1 loss: 1.85047 duration: 4.631187200546265\n",
      "Epoch: 6 finished! Average loss is 1.8545871973 Average time is 5.043644547462463\n",
      "Step: 0 loss: 1.76047 duration: 4.552552938461304\n",
      "Step: 1 loss: 1.73814 duration: 4.658047914505005\n",
      "Epoch: 7 finished! Average loss is 1.74930691719 Average time is 4.605300426483154\n",
      "Step: 0 loss: 1.65518 duration: 4.578516244888306\n",
      "Step: 1 loss: 1.61339 duration: 4.627441883087158\n",
      "Epoch: 8 finished! Average loss is 1.63428586721 Average time is 4.602979063987732\n",
      "Step: 0 loss: 1.53362 duration: 4.536704063415527\n",
      "Step: 1 loss: 1.51975 duration: 4.656726837158203\n",
      "Epoch: 9 finished! Average loss is 1.52668207884 Average time is 4.596715450286865\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fopen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-09a264f0c1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mfilebytearray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sound.raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilebytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fopen' is not defined"
     ]
    }
   ],
   "source": [
    "# trening sekcija\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Pokretanje incijalizatora\n",
    "    sess.run(init)\n",
    "    \n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "    \n",
    "    average_loss = 0.\n",
    "    average_duration = 0.\n",
    "    \n",
    "    while current_epoch < 10:\n",
    "        \n",
    "        e, batch_x, batch_y = ds.next_minibatch()\n",
    "        \n",
    "        x_oh = one_hot(batch_x, quantization_channels)\n",
    "        y_oh = one_hot(batch_y, quantization_channels)\n",
    "        y_oh = y_oh[:, receptive_field:, :]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        _, loss_ = sess.run([train_op, loss], feed_dict={x: x_oh, y: y_oh})\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        average_loss += loss_\n",
    "        average_duration += duration\n",
    "        \n",
    "        print('Step:', batch, 'loss:', loss_, \"duration:\", duration)\n",
    "        \n",
    "        batch += 1\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            print(\"Epoch:\", current_epoch-1, \"finished! Average loss is\", average_loss/batch, \"Average time is\", average_duration/batch)\n",
    "            batch = 0\n",
    "            average_loss = 0.\n",
    "            average_duration = 0.\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    # OD TUDA NA DOLJE IDE GENERIRANJE\n",
    "    ###############################################################\n",
    "        \n",
    "    e, batch_x, batch_y = ds.next_minibatch()\n",
    "    size = batch_x.shape[1]\n",
    "    x_oh = one_hot(batch_x, quantization_channels)\n",
    "    x_oh = x_oh[0, :, :]\n",
    "    x_oh = x_oh.reshape(1, size, quantization_channels)\n",
    "    \n",
    "    sound = []\n",
    "    \n",
    "    for i in range(16000):\n",
    "        #print(i)\n",
    "        #start2 = time.time()\n",
    "        #start = time.time()\n",
    "        predi = sess.run([prediction], feed_dict={x: x_oh})\n",
    "        #print(time.time() - start)\n",
    "        #start = time.time()\n",
    "        x_oh[:, :-1, :] = x_oh[:, 1:, :]\n",
    "        #print(time.time() - start)\n",
    "        reza = predi[0]\n",
    "        x_oh[:, -1, :] = reza[:, -1, :]\n",
    "        sound.append(np.argmax(reza[0, -1, :]))\n",
    "        #print(time.time() - start2)\n",
    "    \n",
    "    filebytearray = bytearray(sound)\n",
    "    \n",
    "    file = fopen(\"sound.raw\", \"wb\")\n",
    "    \n",
    "    file.write(filebytearray)\n",
    "    file.close()\n",
    "    \n",
    "    print('Optimization finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fi = open(\"sound.raw\", \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fi.write(filebytearray)\n",
    "fi.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
